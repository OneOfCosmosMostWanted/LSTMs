{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPd5Y8ukgq1Znkb3mN+zPBg"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://towardsdatascience.com/building-a-lstm-by-hand-on-pytorch-59c02a4ec091\n",
        "\n",
        "\n",
        "https://medium.com/@wangdk93/lstm-from-scratch-c8b4baf06a8b\n",
        "\n",
        "\n",
        "https://towardsdatascience.com/building-a-lstm-by-hand-on-pytorch-59c02a4ec091\n"
      ],
      "metadata": {
        "id": "TgBuFg_eFB0-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUeU36ZJm6qE"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NaiveCustomLSTM(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_sz: int,\n",
        "                 hidden_sz: int):\n",
        "\n",
        "        super().__init__()\n",
        "        self.input_size = input_sz\n",
        "        self.hidden_size = hidden_sz\n",
        "\n",
        "        #i_t\n",
        "        self.U_i = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
        "        self.V_i = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
        "        self.b_i = nn.Parameter(torch.Tensor(hidden_sz))\n",
        "\n",
        "        #f_t\n",
        "        self.U_f = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
        "        self.V_f = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
        "        self.b_f = nn.Parameter(torch.Tensor(hidden_sz))\n",
        "\n",
        "        #c_t\n",
        "        self.U_c = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
        "        self.V_c = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
        "        self.b_c = nn.Parameter(torch.Tensor(hidden_sz))\n",
        "\n",
        "        #o_t\n",
        "        self.U_o = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
        "        self.V_o = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
        "        self.b_o = nn.Parameter(torch.Tensor(hidden_sz))\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
        "        for weight in self.parameters():\n",
        "            weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self,\n",
        "                x,\n",
        "                init_states=None):\n",
        "\n",
        "        \"\"\"\n",
        "        assumes x.shape represents (batch_size, sequence_size, input_size)\n",
        "        \"\"\"\n",
        "        bs, seq_sz, _ = x.size()\n",
        "        hidden_seq = []\n",
        "\n",
        "        if init_states is None:\n",
        "            h_t, c_t = (\n",
        "                torch.zeros(bs, self.hidden_size).to(x.device),\n",
        "                torch.zeros(bs, self.hidden_size).to(x.device),\n",
        "            )\n",
        "        else:\n",
        "            h_t, c_t = init_states\n",
        "\n",
        "        for t in range(seq_sz):\n",
        "            x_t = x[:, t, :]\n",
        "\n",
        "            i_t = torch.sigmoid(x_t @ self.U_i + h_t @ self.V_i + self.b_i)\n",
        "            f_t = torch.sigmoid(x_t @ self.U_f + h_t @ self.V_f + self.b_f)\n",
        "            g_t = torch.tanh(x_t @ self.U_c + h_t @ self.V_c + self.b_c)\n",
        "            o_t = torch.sigmoid(x_t @ self.U_o + h_t @ self.V_o + self.b_o)\n",
        "            c_t = f_t * c_t + i_t * g_t\n",
        "            h_t = o_t * torch.tanh(c_t)\n",
        "\n",
        "            hidden_seq.append(h_t.unsqueeze(0))\n",
        "\n",
        "        #reshape hidden_seq p/ retornar\n",
        "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
        "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
        "        return hidden_seq, (h_t, c_t)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "1PXE5SVQnFDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomLSTM(nn.Module):\n",
        "    def __init__(self, input_sz, hidden_sz):\n",
        "        super().__init__()\n",
        "        self.input_sz = input_sz\n",
        "        self.hidden_size = hidden_sz\n",
        "        self.W = nn.Parameter(torch.Tensor(input_sz, hidden_sz * 4))\n",
        "        self.U = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz * 4))\n",
        "        self.bias = nn.Parameter(torch.Tensor(hidden_sz * 4))\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
        "        for weight in self.parameters():\n",
        "            weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, x,\n",
        "                init_states=None):\n",
        "        \"\"\"Assumes x is of shape (batch, sequence, feature)\"\"\"\n",
        "        bs, seq_sz, _ = x.size()\n",
        "        hidden_seq = []\n",
        "        if init_states is None:\n",
        "            h_t, c_t = (torch.zeros(bs, self.hidden_size).to(x.device),\n",
        "                        torch.zeros(bs, self.hidden_size).to(x.device))\n",
        "        else:\n",
        "            h_t, c_t = init_states\n",
        "\n",
        "        HS = self.hidden_size\n",
        "        for t in range(seq_sz):\n",
        "            x_t = x[:, t, :]\n",
        "            # batch the computations into a single matrix multiplication\n",
        "            gates = x_t @ self.W + h_t @ self.U + self.bias\n",
        "            i_t, f_t, g_t, o_t = (\n",
        "                torch.sigmoid(gates[:, :HS]), # input\n",
        "                torch.sigmoid(gates[:, HS:HS*2]), # forget\n",
        "                torch.tanh(gates[:, HS*2:HS*3]),\n",
        "                torch.sigmoid(gates[:, HS*3:]), # output\n",
        "            )\n",
        "            c_t = f_t * c_t + i_t * g_t\n",
        "            h_t = o_t * torch.tanh(c_t)\n",
        "            hidden_seq.append(h_t.unsqueeze(0))\n",
        "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
        "        # reshape from shape (sequence, batch, feature) to (batch, sequence, feature)\n",
        "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
        "        return hidden_seq, (h_t, c_t)"
      ],
      "metadata": {
        "id": "sbhX6eVwnupU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Peephole_LSTM(nn.Module):\n",
        "    def __init__(self, input_sz, hidden_sz, peephole=False):\n",
        "        super().__init__()\n",
        "        self.input_sz = input_sz\n",
        "        self.hidden_size = hidden_sz\n",
        "        self.peephole = peephole\n",
        "        self.W = nn.Parameter(torch.Tensor(input_sz, hidden_sz * 4))\n",
        "        self.U = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz * 4))\n",
        "        self.bias = nn.Parameter(torch.Tensor(hidden_sz * 4))\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
        "        for weight in self.parameters():\n",
        "            weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, x,\n",
        "                init_states=None):\n",
        "        \"\"\"Assumes x is of shape (batch, sequence, feature)\"\"\"\n",
        "        bs, seq_sz, _ = x.size()\n",
        "        hidden_seq = []\n",
        "        if init_states is None:\n",
        "            h_t, c_t = (torch.zeros(bs, self.hidden_size).to(x.device),\n",
        "                        torch.zeros(bs, self.hidden_size).to(x.device))\n",
        "        else:\n",
        "            h_t, c_t = init_states\n",
        "\n",
        "        HS = self.hidden_size\n",
        "        for t in range(seq_sz):\n",
        "            x_t = x[:, t, :]\n",
        "            # batch the computations into a single matrix multiplication\n",
        "\n",
        "            if self.peephole:\n",
        "                gates = x_t @ U + c_t @ V + bias\n",
        "            else:\n",
        "                gates = x_t @ U + h_t @ V + bias\n",
        "                g_t = torch.tanh(gates[:, HS*2:HS*3])\n",
        "\n",
        "            i_t, f_t, o_t = (\n",
        "                torch.sigmoid(gates[:, :HS]), # input\n",
        "                torch.sigmoid(gates[:, HS:HS*2]), # forget\n",
        "                torch.sigmoid(gates[:, HS*3:]), # output\n",
        "            )\n",
        "\n",
        "            if self.peephole:\n",
        "                c_t = f_t * c_t + i_t * torch.sigmoid(x_t @ U + bias)[:, HS*2:HS*3]\n",
        "                h_t = torch.tanh(o_t * c_t)\n",
        "            else:\n",
        "                c_t = f_t * c_t + i_t * g_t\n",
        "                h_t = o_t * torch.tanh(c_t)\n",
        "\n",
        "            hidden_seq.append(h_t.unsqueeze(0))\n",
        "\n",
        "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
        "        # reshape from shape (sequence, batch, feature) to (batch, sequence, feature)\n",
        "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
        "\n",
        "        return hidden_seq, (h_t, c_t)"
      ],
      "metadata": {
        "id": "243gsH2XE5GL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class LSTM_cell_AI_SUMMER(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    A simple LSTM cell network for educational AI-summer purposes\n",
        "    \"\"\"\n",
        "    def __init__(self, input_length=10, hidden_length=20):\n",
        "        super(LSTM_cell_AI_SUMMER, self).__init__()\n",
        "        self.input_length = input_length\n",
        "        self.hidden_length = hidden_length\n",
        "\n",
        "        # forget gate components\n",
        "        self.linear_forget_w1 = nn.Linear(self.input_length, self.hidden_length, bias=True)\n",
        "        self.linear_forget_r1 = nn.Linear(self.hidden_length, self.hidden_length, bias=False)\n",
        "        self.sigmoid_forget = nn.Sigmoid()\n",
        "\n",
        "        # input gate components\n",
        "        self.linear_gate_w2 = nn.Linear(self.input_length, self.hidden_length, bias=True)\n",
        "        self.linear_gate_r2 = nn.Linear(self.hidden_length, self.hidden_length, bias=False)\n",
        "        self.sigmoid_gate = nn.Sigmoid()\n",
        "\n",
        "        # cell memory components\n",
        "        self.linear_gate_w3 = nn.Linear(self.input_length, self.hidden_length, bias=True)\n",
        "        self.linear_gate_r3 = nn.Linear(self.hidden_length, self.hidden_length, bias=False)\n",
        "        self.activation_gate = nn.Tanh()\n",
        "\n",
        "        # out gate components\n",
        "        self.linear_gate_w4 = nn.Linear(self.input_length, self.hidden_length, bias=True)\n",
        "        self.linear_gate_r4 = nn.Linear(self.hidden_length, self.hidden_length, bias=False)\n",
        "        self.sigmoid_hidden_out = nn.Sigmoid()\n",
        "\n",
        "        self.activation_final = nn.Tanh()\n",
        "\n",
        "    def forget(self, x, h):\n",
        "        x = self.linear_forget_w1(x)\n",
        "        h = self.linear_forget_r1(h)\n",
        "        return self.sigmoid_forget(x + h)\n",
        "\n",
        "    def input_gate(self, x, h):\n",
        "\n",
        "        # Equation 1. input gate\n",
        "        x_temp = self.linear_gate_w2(x)\n",
        "        h_temp = self.linear_gate_r2(h)\n",
        "        i = self.sigmoid_gate(x_temp + h_temp)\n",
        "        return i\n",
        "\n",
        "    def cell_memory_gate(self, i, f, x, h, c_prev):\n",
        "        x = self.linear_gate_w3(x)\n",
        "        h = self.linear_gate_r3(h)\n",
        "\n",
        "        # new information part that will be injected in the new context\n",
        "        k = self.activation_gate(x + h)\n",
        "        g = k * i\n",
        "\n",
        "        # forget old context/cell info\n",
        "        c = f * c_prev\n",
        "        # learn new context/cell info\n",
        "        c_next = g + c\n",
        "        return c_next\n",
        "\n",
        "    def out_gate(self, x, h):\n",
        "        x = self.linear_gate_w4(x)\n",
        "        h = self.linear_gate_r4(h)\n",
        "        return self.sigmoid_hidden_out(x + h)\n",
        "\n",
        "    def forward(self, x, tuple_in ):\n",
        "        (h, c_prev) = tuple_in\n",
        "        # Equation 1. input gate\n",
        "        i = self.input_gate(x, h)\n",
        "\n",
        "        # Equation 2. forget gate\n",
        "        f = self.forget(x, h)\n",
        "\n",
        "        # Equation 3. updating the cell memory\n",
        "        c_next = self.cell_memory_gate(i, f, x, h,c_prev)\n",
        "\n",
        "        # Equation 4. calculate the main output gate\n",
        "        o = self.out_gate(x, h)\n",
        "\n",
        "\n",
        "        # Equation 5. produce next hidden output\n",
        "        h_next = o * self.activation_final(c_next)\n",
        "\n",
        "        return h_next, c_next"
      ],
      "metadata": {
        "id": "8-RX119FT-xS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}